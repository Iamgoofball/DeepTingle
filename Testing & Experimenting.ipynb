{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys;\n",
    "import re;\n",
    "import operator;\n",
    "import numpy as np;\n",
    "from os import listdir;\n",
    "from keras.utils import np_utils;\n",
    "from keras.regularizers import l2;\n",
    "from keras.preprocessing.text import Tokenizer;\n",
    "from keras.preprocessing.sequence import pad_sequences;\n",
    "from keras.callbacks import ModelCheckpoint;\n",
    "from keras import backend as K;\n",
    "from keras.engine.topology import Layer;\n",
    "from keras.models import Sequential;\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, TimeDistributed, Input;\n",
    "from keras.optimizers import Adam;\n",
    "\n",
    "def cleanText(text):\n",
    "    modifiedString = re.sub(\"\\d\", \"\", text);\n",
    "    modifiedString = modifiedString.lower();\n",
    "    modifiedString = modifiedString.replace(\"\\t\", \" \");\n",
    "    modifiedString = modifiedString.replace(\"\\n\", \" \");\n",
    "    modifiedString = modifiedString.replace('!', \" . \");\n",
    "    modifiedString = modifiedString.replace('\"', \" \");\n",
    "    modifiedString = modifiedString.replace('#', \" \"); \n",
    "    modifiedString = modifiedString.replace(\"'\", \"'\"); \n",
    "    modifiedString = modifiedString.replace('(', \" \"); \n",
    "    modifiedString = modifiedString.replace(')', \" \"); \n",
    "    modifiedString = modifiedString.replace(',', \" \");\n",
    "    modifiedString = modifiedString.replace('-', \" \"); \n",
    "    modifiedString = modifiedString.replace('.', \" . \"); \n",
    "    modifiedString = modifiedString.replace('/', \" \");\n",
    "    modifiedString = modifiedString.replace(':', \" \");\n",
    "    modifiedString = modifiedString.replace(';', \" . \"); \n",
    "    modifiedString = modifiedString.replace('?', \" . \");\n",
    "    modifiedString = modifiedString.replace('–', \" \");\n",
    "    modifiedString = modifiedString.replace('—', \" \");\n",
    "    modifiedString = modifiedString.replace('‘', \"'\"); \n",
    "    modifiedString = modifiedString.replace('…', \" . \");\n",
    "    modifiedString = modifiedString.replace('ç', \"c\");\n",
    "    modifiedString = modifiedString.replace('é', \"e\");\n",
    "    \n",
    "    return modifiedString;\n",
    "\n",
    "def generateData(batchSize, dataX, dataY):\n",
    "    while True:\n",
    "        randomStart = np.random.randint(len(dataX) - batchSize);\n",
    "        X = dataX[randomStart:randomStart+batchSize];\n",
    "        Y = dataY[randomStart:randomStart+batchSize];\n",
    "        X = tokenizer.texts_to_sequences(dataX[randomStart:randomStart+batchSize]);\n",
    "        Y = tokenizer.texts_to_sequences(dataY[randomStart:randomStart+batchSize]);\n",
    "        X = pad_sequences(X, maxlen=timeSeriesLength);\n",
    "        for temp in Y:\n",
    "            if len(temp) == 0:\n",
    "                temp.append(0);\n",
    "        Y = np_utils.to_categorical(Y, nb_classes=len(tokenizer.word_index) + 1);\n",
    "        yield (X, Y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading tingle stories\n",
      "20856 sentences has been loaded\n",
      "134 stories has been loaded\n",
      "\n",
      "Converting input data to sequences...\n",
      "Number of stories patterns: 586453\n",
      "Number of sentence patterns: 308268\n",
      "\n",
      "Reshaping the data\n",
      "Number of words repeated more than 2 times are 8088 while 4352 non common\n",
      "\n",
      "Loading GloVe\n",
      "Found 12450 word vectors.\n",
      "\n",
      "Constructing Embedding Matrix\n",
      "Embedding Matrix Loaded of Dimensions: (12441, 100)\n",
      "0 out of 12440 is not in corpus\n"
     ]
    }
   ],
   "source": [
    "timeSeriesLength = 100;\n",
    "minNumberOfWords = 10;\n",
    "path = \"Stories/\";\n",
    "print(\"Reading tingle stories\");\n",
    "files = [f for f in listdir(path) if (\"DS_Store\" not in f) and (\"ipynb\" not in f)];\n",
    "rawData = [];\n",
    "rawSentences = [];\n",
    "for f in files:\n",
    "    text = open(path + f).read();\n",
    "    text = cleanText(text);\n",
    "    sent = text.split(\".\");\n",
    "    for s in sent:\n",
    "        if len(s.strip().split()) > minNumberOfWords:\n",
    "            rawSentences.append(s);\n",
    "    rawData.append(text);\n",
    "print(\"%d sentences has been loaded\"%len(rawSentences));\n",
    "print(\"%d stories has been loaded\"%len(files));\n",
    "\n",
    "print(\"\\nConverting input data to sequences...\");\n",
    "dataX = [];\n",
    "dataY = [];\n",
    "for t in rawData:\n",
    "    words = t.split();\n",
    "    for i in range(0, timeSeriesLength):\n",
    "        dataX.append(\" \".join(words[0:i]));\n",
    "        dataY.append(words[i]);\n",
    "    for i in range(timeSeriesLength, len(words) - timeSeriesLength, 1):\n",
    "        finalIndex = i+timeSeriesLength;\n",
    "        if i + timeSeriesLength >= len(words):\n",
    "            finalIndex = len(words) - 1;\n",
    "        dataX.append(\" \".join(words[i:finalIndex]));\n",
    "        dataY.append(words[finalIndex]);\n",
    "\n",
    "sentX = [];\n",
    "sentY = [];\n",
    "for sent in rawSentences:\n",
    "    words = sent.split();\n",
    "    words.append(\".\");\n",
    "    for i in range(0, min(timeSeriesLength, minNumberOfWords)):\n",
    "        sentX.append(\" \".join(words[0:i]));\n",
    "        sentY.append(words[i]);\n",
    "    for i in range(min(timeSeriesLength, minNumberOfWords), len(words) - min(timeSeriesLength, minNumberOfWords), 1):\n",
    "        finalIndex = i+timeSeriesLength;\n",
    "        if i + timeSeriesLength >= len(words):\n",
    "            finalIndex = len(words) - 1;\n",
    "        sentX.append(\" \".join(words[i:finalIndex]));\n",
    "        sentY.append(words[finalIndex]);\n",
    "print(\"Number of stories patterns: %d\"%len(dataX));\n",
    "print(\"Number of sentence patterns: %d\"%len(sentX));\n",
    "\n",
    "print(\"\\nReshaping the data\");\n",
    "minNumberOfRepeated = 2;\n",
    "tokenizer = Tokenizer(filters=\"\");\n",
    "tokenizer.fit_on_texts(rawData);\n",
    "wordCountDictionary = tokenizer.word_counts;\n",
    "wordCountDictionary = sorted(wordCountDictionary.items(), key=operator.itemgetter(1), reverse=True);\n",
    "listOfWords = [];\n",
    "nonCommonWords = [];\n",
    "for w,c in wordCountDictionary:\n",
    "    if c > minNumberOfRepeated:\n",
    "        listOfWords.append(w);\n",
    "    else:\n",
    "        nonCommonWords.append(w);\n",
    "print(\"Number of words repeated more than %d times are %d while %d non common\"%(minNumberOfRepeated, len(listOfWords), len(nonCommonWords)));\n",
    "\n",
    "print(\"\\nLoading GloVe\");\n",
    "wordDimensions = 100;\n",
    "glovePath = \"GloveData/\";\n",
    "embeddingDic = {};\n",
    "f = open(glovePath + \"tingle-vectors-\" + str(wordDimensions) + \".txt\");\n",
    "for line in f:\n",
    "    values = line.split();\n",
    "    currrentWord = values[0];\n",
    "    currentVector = np.asarray(values[1:], dtype='float32');\n",
    "    embeddingDic[currrentWord] = currentVector;\n",
    "f.close();\n",
    "print('Found %d word vectors.' % len(embeddingDic));\n",
    "\n",
    "print(\"\\nConstructing Embedding Matrix\");\n",
    "embeddingMatrix = np.zeros((len(tokenizer.word_index) + 1, wordDimensions));\n",
    "wordsNotInWiki = [];\n",
    "reverseDic = {};\n",
    "for w, i in tokenizer.word_index.items():\n",
    "    reverseDic[i] = w;\n",
    "    if w in embeddingDic:\n",
    "        embeddingMatrix[i] = embeddingDic[w];\n",
    "    else:\n",
    "        wordsNotInWiki.append(w);\n",
    "print(\"Embedding Matrix Loaded of Dimensions: \" + str(embeddingMatrix.shape));\n",
    "print(\"%d out of %d is not in corpus\"%(len(wordsNotInWiki), len(tokenizer.word_index)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KNNLayer(Layer):\n",
    "    def __init__(self, weights, **kwargs):\n",
    "        self.output_dim = weights.shape[1];\n",
    "        self.W = K.variable(weights);\n",
    "        super(KNNLayer, self).__init__(**kwargs);\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(KNNLayer, self).build(input_shape);\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        print(x);\n",
    "#         newX = K.transpose(K.repeat_elements(x, self.output_dim, axis=0));\n",
    "        return K.dot(x, self.W)\n",
    "#         return K.sum(K.pow(newX - self.W, 2), axis=0);\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The first layer in a Sequential model must get an `input_shape` or `batch_input_shape` argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-9cb597d83aed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKNNLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddingMatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/jupyter/anaconda3/envs/py35/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0;31m# create an input layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch_input_shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m                     raise ValueError('The first layer in a '\n\u001b[0m\u001b[1;32m    291\u001b[0m                                      \u001b[0;34m'Sequential model must '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                                      \u001b[0;34m'get an `input_shape` or '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The first layer in a Sequential model must get an `input_shape` or `batch_input_shape` argument."
     ]
    }
   ],
   "source": [
    "model = Sequential();\n",
    "\n",
    "model.add(KNNLayer(np.transpose(np.array(embeddingMatrix))));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n",
      "fierceness that sends shivers down my spine . \" travis , will you marry me ? \" my own sentient investment strategy asks . \" of course i will , \" i tell him . \" of course i will . \"\n"
     ]
    }
   ],
   "source": [
    "wordSequence = [];\n",
    "differentWords = rawData.split();\n",
    "for i in range(0, len(differentWords), 100):\n",
    "    if i < len(differentWords) - 100:\n",
    "        wordSequence.append(\" \".join(differentWords[i:i+100]));\n",
    "    else:\n",
    "        wordSequence.append(\" \".join(differentWords[i:len(differentWords)]));\n",
    "print(len(wordSequence[-1]));\n",
    "print(wordSequence[-1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22976\n",
      "557976\n",
      "(3044078, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "print(len(uniqueWords));\n",
    "print(len(rawData.split()));\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(wordSequence)\n",
    "sequences = tokenizer.texts_to_sequences(wordSequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "[11, 43, 7, 5, 345, 14, 64, 8973, 3981, 12, 52, 146, 164, 19, 1696, 6, 2, 800, 146, 816, 12, 4, 39, 164, 2, 4266, 36, 1, 241, 7, 9378, 1752, 3, 25, 19, 213, 4432, 7, 9629, 39, 5899, 47, 4663, 1609, 5401, 6616, 6, 464, 933, 8345, 28, 581, 9, 3, 16, 40, 2, 52, 544, 69, 2, 52, 5099, 22, 50, 70, 2, 137, 24, 1, 262, 7, 5, 432, 11, 717, 1598, 474, 11, 8, 2738, 310, 670, 97, 2, 763, 4015, 28, 1, 191]\n",
      "12068\n",
      "100\n",
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0 11069\n",
      "     9  2058 11028    31     5   766  5914   165    13  4833    10     5\n",
      "   135   588  3809  3204   156     7   245     2   165     2    67    32\n",
      "     7   245     2   165]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "print(len(sequences[0]));\n",
    "print(sequences[0])\n",
    "print(len(tokenizer.word_index.items()))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=100)\n",
    "print(len(data[0]))\n",
    "print(data[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00022508757056577487\n"
     ]
    }
   ],
   "source": [
    "count = 0;\n",
    "for i in range(len(rawData) - 1):\n",
    "    if rawData[i] == rawData[i + 1] and rawData[i] == \" \":\n",
    "        count += 1;\n",
    "        \n",
    "print(str(count / float(len(rawData))));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential;\n",
    "from keras.layers import Dense, LSTM, Dropout;\n",
    "import sys\n",
    "import numpy\n",
    "\n",
    "numOfCharacters = 1000;\n",
    "\n",
    "model = Sequential();\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256));\n",
    "model.add(Dropout(0.2));\n",
    "model.add(Dense(Y.shape[1], activation='softmax'))\n",
    "model.load_weights(\"AliceWeights/alltingle-weights-improvement-21-1.3086.hdf5\");\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\");\n",
    "startText = startingData[np.random.randint(0, len(startingData))];\n",
    "pattern = [charInt[c] for c in startText];\n",
    "result = startText + \"\";\n",
    "print(\"Starting Text: \\\"\" + startText + \"\\\"\");\n",
    "\n",
    "i = 0;\n",
    "while i < numOfCharacters:\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(numChars)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = intChar[index]\n",
    "    seq_in = [intChar[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "    i+= 1;\n",
    "    \n",
    "print(\"The end\\n\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
