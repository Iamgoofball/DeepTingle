{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134 stories has been loaded\n",
      "Number of different characters: 50\n",
      "Different characters: ['\\n', ' ', '!', '\"', '#', \"'\", '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '7', '8', '9', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Number of characters in the text: 3041774\n",
      "Converting input data to sequences...\n",
      "All data has been converted...\n",
      "Number of patterns: 3041674\n",
      "Reshaping the data\n"
     ]
    }
   ],
   "source": [
    "from os import listdir;\n",
    "import numpy as np;\n",
    "from keras.utils import np_utils;\n",
    "import sys;\n",
    "\n",
    "seqLength = 100;\n",
    "path = \"Stories/\";\n",
    "files = [f for f in listdir(path) if (\".DS_Store\" not in f) and (\".ipynb_checkpoints\" not in f)];\n",
    "rawData = \"\";\n",
    "startingData = [];\n",
    "uniqueWords = [];\n",
    "for f in files:\n",
    "    text = open(path + f).read();\n",
    "    \n",
    "    text = text.replace(\"\\t\", \" \");\n",
    "#     text = text.replace(\"\\n\", \" \\n\");\n",
    "#     text = text.replace('!', \" ! \");\n",
    "#     text = text.replace('\"', ' \" ');\n",
    "#     text = text.replace('#', \" # \"); \n",
    "#     text = text.replace(\"'\", \" ' \"); \n",
    "#     text = text.replace('(', \" ) \"); \n",
    "#     text = text.replace(')', \" ) \"); \n",
    "#     text = text.replace(',', \" , \");\n",
    "#     text = text.replace('-', \" - \"); \n",
    "#     text = text.replace('.', \" . \"); \n",
    "#     text = text.replace('/', \" / \");\n",
    "#     text = text.replace(':', \" : \");\n",
    "#     text = text.replace(';', \" ; \"); \n",
    "#     text = text.replace('?', \" ? \");\n",
    "    text = text.replace('–', \" - \");\n",
    "    text = text.replace('—', \" - \");\n",
    "    text = text.replace('‘', \" ' \"); \n",
    "    text = text.replace('…', \" . \");\n",
    "    text = text.replace('ç', \"c\");\n",
    "    text = text.replace('é', \"e\");\n",
    "    text = text.lower();\n",
    "    \n",
    "    if \"dan bigfootzerian parties in my butthole with his billionaire lifestyle\" in text:\n",
    "        print(f);\n",
    "    \n",
    "    words = text.split();\n",
    "    for w in words:\n",
    "        if w not in uniqueWords:\n",
    "            uniqueWords.append(w)\n",
    "    \n",
    "    rawData += text;\n",
    "    startingData.append(text[0:seqLength]);\n",
    "    \n",
    "chars = sorted(list(set(rawData)));\n",
    "numChars = len(chars);\n",
    "numData = len(rawData);\n",
    "\n",
    "print(\"%d stories has been loaded\"%len(files));\n",
    "print(\"Number of different characters: %d\"%numChars);\n",
    "print(\"Different characters: \" + str(chars));\n",
    "print(\"Number of characters in the text: %d\"%numData);\n",
    "\n",
    "charInt = dict((c, i) for i,c in enumerate(chars));\n",
    "intChar = dict((i, c) for i,c in enumerate(chars));\n",
    "\n",
    "print(\"Converting input data to sequences...\");\n",
    "dataX = [];\n",
    "dataY = [];\n",
    "for i in range(numData - seqLength):\n",
    "    seqIn = rawData[i:i+seqLength];\n",
    "    seqOut = rawData[i+seqLength];\n",
    "    \n",
    "    dataX.append([charInt[c] for c in seqIn]);\n",
    "    dataY.append(charInt[seqOut]);\n",
    "    \n",
    "numPatterns = len(dataX);\n",
    "print(\"All data has been converted...\");\n",
    "print(\"Number of patterns: %d\"%numPatterns);\n",
    "print(\"Reshaping the data\");\n",
    "X = np.reshape(dataX, (numPatterns, seqLength, 1));\n",
    "X = X/float(numChars);\n",
    "Y = np_utils.to_categorical(dataY);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1037\n"
     ]
    }
   ],
   "source": [
    "sents = rawData.split(\".\");\n",
    "print((max(sents, key=len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n",
      "fierceness that sends shivers down my spine . \" travis , will you marry me ? \" my own sentient investment strategy asks . \" of course i will , \" i tell him . \" of course i will . \"\n"
     ]
    }
   ],
   "source": [
    "wordSequence = [];\n",
    "differentWords = rawData.split();\n",
    "for i in range(0, len(differentWords), 100):\n",
    "    if i < len(differentWords) - 100:\n",
    "        wordSequence.append(\" \".join(differentWords[i:i+100]));\n",
    "    else:\n",
    "        wordSequence.append(\" \".join(differentWords[i:len(differentWords)]));\n",
    "print(len(wordSequence[-1]));\n",
    "print(wordSequence[-1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22976\n",
      "557976\n",
      "(3044078, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "print(len(uniqueWords));\n",
    "print(len(rawData.split()));\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(wordSequence)\n",
    "sequences = tokenizer.texts_to_sequences(wordSequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "[11, 43, 7, 5, 345, 14, 64, 8973, 3981, 12, 52, 146, 164, 19, 1696, 6, 2, 800, 146, 816, 12, 4, 39, 164, 2, 4266, 36, 1, 241, 7, 9378, 1752, 3, 25, 19, 213, 4432, 7, 9629, 39, 5899, 47, 4663, 1609, 5401, 6616, 6, 464, 933, 8345, 28, 581, 9, 3, 16, 40, 2, 52, 544, 69, 2, 52, 5099, 22, 50, 70, 2, 137, 24, 1, 262, 7, 5, 432, 11, 717, 1598, 474, 11, 8, 2738, 310, 670, 97, 2, 763, 4015, 28, 1, 191]\n",
      "12068\n",
      "100\n",
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0 11069\n",
      "     9  2058 11028    31     5   766  5914   165    13  4833    10     5\n",
      "   135   588  3809  3204   156     7   245     2   165     2    67    32\n",
      "     7   245     2   165]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "print(len(sequences[0]));\n",
    "print(sequences[0])\n",
    "print(len(tokenizer.word_index.items()))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=100)\n",
    "print(len(data[0]))\n",
    "print(data[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00022508757056577487\n"
     ]
    }
   ],
   "source": [
    "count = 0;\n",
    "for i in range(len(rawData) - 1):\n",
    "    if rawData[i] == rawData[i + 1] and rawData[i] == \" \":\n",
    "        count += 1;\n",
    "        \n",
    "print(str(count / float(len(rawData))));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential;\n",
    "from keras.layers import Dense, LSTM, Dropout;\n",
    "import sys\n",
    "import numpy\n",
    "\n",
    "numOfCharacters = 1000;\n",
    "\n",
    "model = Sequential();\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256));\n",
    "model.add(Dropout(0.2));\n",
    "model.add(Dense(Y.shape[1], activation='softmax'))\n",
    "model.load_weights(\"AliceWeights/alltingle-weights-improvement-21-1.3086.hdf5\");\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\");\n",
    "startText = startingData[np.random.randint(0, len(startingData))];\n",
    "pattern = [charInt[c] for c in startText];\n",
    "result = startText + \"\";\n",
    "print(\"Starting Text: \\\"\" + startText + \"\\\"\");\n",
    "\n",
    "i = 0;\n",
    "while i < numOfCharacters:\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(numChars)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = intChar[index]\n",
    "    seq_in = [intChar[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "    i+= 1;\n",
    "    \n",
    "print(\"The end\\n\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
