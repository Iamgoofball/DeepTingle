{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Libraries and Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys;\n",
    "import re;\n",
    "import operator;\n",
    "import numpy as np;\n",
    "import math;\n",
    "import random;\n",
    "from os import listdir, environ;\n",
    "from keras.utils import np_utils;\n",
    "from keras.regularizers import l2;\n",
    "from keras.preprocessing.text import Tokenizer;\n",
    "from keras.preprocessing.sequence import pad_sequences;\n",
    "from keras.callbacks import ModelCheckpoint;\n",
    "from keras.models import Sequential;\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, TimeDistributed, Input;\n",
    "from keras.layers.wrappers import Bidirectional;\n",
    "from keras.optimizers import Adam;\n",
    "\n",
    "def cleanText(text):\n",
    "    modifiedString = re.sub(\"\\d\", \"\", text);\n",
    "    modifiedString = modifiedString.lower();\n",
    "    modifiedString = modifiedString.replace(\"\\t\", \" \");\n",
    "    modifiedString = modifiedString.replace(\"\\n\", \" \");\n",
    "    modifiedString = modifiedString.replace('!', \" . \");\n",
    "    modifiedString = modifiedString.replace('\"', \" \");\n",
    "    modifiedString = modifiedString.replace('#', \" \"); \n",
    "    modifiedString = modifiedString.replace(\"'\", \"'\"); \n",
    "    modifiedString = modifiedString.replace('(', \" \"); \n",
    "    modifiedString = modifiedString.replace(')', \" \"); \n",
    "    modifiedString = modifiedString.replace(',', \" , \");\n",
    "    modifiedString = modifiedString.replace('-', \" \"); \n",
    "    modifiedString = modifiedString.replace('.', \" . \"); \n",
    "    modifiedString = modifiedString.replace('/', \" \");\n",
    "    modifiedString = modifiedString.replace(':', \" \");\n",
    "    modifiedString = modifiedString.replace(';', \" ; \"); \n",
    "    modifiedString = modifiedString.replace('?', \" ? \");\n",
    "    modifiedString = modifiedString.replace('–', \" \");\n",
    "    modifiedString = modifiedString.replace('—', \" \");\n",
    "    modifiedString = modifiedString.replace('‘', \"'\"); \n",
    "    modifiedString = modifiedString.replace('…', \" . \");\n",
    "    modifiedString = modifiedString.replace('ç', \"c\");\n",
    "    modifiedString = modifiedString.replace('é', \"e\");\n",
    "    \n",
    "    return modifiedString;\n",
    "\n",
    "def generateData(batchSize, dataX, dataY):\n",
    "    while True:\n",
    "        randomStart = np.random.randint(math.floor((len(dataX) - batchSize) / timeSeriesLength)) * timeSeriesLength;\n",
    "        X = dataX[randomStart:randomStart+batchSize];\n",
    "        Y = dataY[randomStart:randomStart+batchSize];\n",
    "        X = tokenizer.texts_to_sequences(dataX[randomStart:randomStart+batchSize]);\n",
    "        Y = tokenizer.texts_to_sequences(dataY[randomStart:randomStart+batchSize]);\n",
    "        X = pad_sequences(X, maxlen=timeSeriesLength);\n",
    "        for temp in Y:\n",
    "            if len(temp) == 0:\n",
    "                temp.append(0);\n",
    "        Y = np_utils.to_categorical(Y, nb_classes=len(tokenizer.word_index) + 1);\n",
    "        yield (X, Y);\n",
    "        \n",
    "def generateNoisyData(batchSize, noiseSize, dataX, dataY):\n",
    "    while True:\n",
    "        randomStart = np.random.randint(math.floor((len(dataX) - batchSize) / timeSeriesLength)) * timeSeriesLength;\n",
    "        X = dataX[randomStart:randomStart+batchSize];\n",
    "        Y = dataY[randomStart:randomStart+batchSize];\n",
    "        X = tokenizer.texts_to_sequences(dataX[randomStart:randomStart+batchSize]);\n",
    "        Y = tokenizer.texts_to_sequences(dataY[randomStart:randomStart+batchSize]);\n",
    "        X = pad_sequences(X, maxlen=timeSeriesLength);\n",
    "        for i in range(len(X)):\n",
    "            array = list(range(0, timeSeriesLength))\n",
    "            random.shuffle(array)\n",
    "            for j in range(noiseSize):\n",
    "                X[i][array[j]] = 0;\n",
    "        for temp in Y:\n",
    "            if len(temp) == 0:\n",
    "                temp.append(0);\n",
    "        Y = np_utils.to_categorical(Y, nb_classes=len(tokenizer.word_index) + 1);\n",
    "        yield (X, Y);\n",
    "        \n",
    "environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batchSize = 100;\n",
    "minNumberOfWords = 10;\n",
    "wordDimensions = 100;\n",
    "\n",
    "def prepareData(timeSeriesLength):\n",
    "    path = \"Stories/\";\n",
    "    print(\"Reading tingle stories\");\n",
    "    files = [f for f in listdir(path) if (\"DS_Store\" not in f) and (\"ipynb\" not in f)];\n",
    "    rawData = [];\n",
    "    rawSentences = [];\n",
    "    rawText = \"\";\n",
    "    for f in files:\n",
    "        text = open(path + f).read();\n",
    "        text = cleanText(text);\n",
    "        sent = re.split(\"\\.|\\?\", text);\n",
    "        for s in sent:\n",
    "            if len(s.strip().split()) > minNumberOfWords:\n",
    "                rawSentences.append(s);\n",
    "        rawData.append(text);\n",
    "        rawText += text;\n",
    "    print(\"%d sentences has been loaded\"%len(rawSentences));\n",
    "    print(\"%d stories has been loaded\"%len(files));\n",
    "\n",
    "    print(\"\\nConverting input data to sequences...\");\n",
    "    dataX = [];\n",
    "    dataY = [];\n",
    "    for t in rawData:\n",
    "        words = t.split();\n",
    "        for k in range(0, timeSeriesLength, 1):\n",
    "            dataX.append(\" \".join(words[0:k]));\n",
    "            dataY.append(words[k]);\n",
    "            for i in range(timeSeriesLength + k, len(words) - timeSeriesLength, timeSeriesLength):\n",
    "                finalIndex = i+timeSeriesLength;\n",
    "                if i + timeSeriesLength >= len(words):\n",
    "                    finalIndex = len(words) - 1;\n",
    "                dataX.append(\" \".join(words[i:finalIndex]));\n",
    "                dataY.append(words[finalIndex]);\n",
    "    \n",
    "    sentX = [];\n",
    "    sentY = [];\n",
    "    for sent in rawSentences:\n",
    "        words = sent.split();\n",
    "        words.append(\".\");\n",
    "        for i in range(0, min(timeSeriesLength, minNumberOfWords)):\n",
    "            sentX.append(\" \".join(words[0:i]));\n",
    "            sentY.append(words[i]);\n",
    "        for i in range(min(timeSeriesLength, minNumberOfWords), len(words) - min(timeSeriesLength, minNumberOfWords), 1):\n",
    "            finalIndex = i+timeSeriesLength;\n",
    "            if i + timeSeriesLength >= len(words):\n",
    "                finalIndex = len(words) - 1;\n",
    "            sentX.append(\" \".join(words[i:finalIndex]));\n",
    "            sentY.append(words[finalIndex]);\n",
    "    print(\"Number of stories patterns: %d\"%len(dataX));\n",
    "    print(\"Number of sentence patterns: %d\"%len(sentX));\n",
    "\n",
    "    print(\"\\nReshaping the data\");\n",
    "    minNumberOfRepeated = 0;\n",
    "    tokenizer = Tokenizer(filters=\"\");\n",
    "    tokenizer.fit_on_texts(rawData);\n",
    "    \n",
    "    wordCountDictionary = tokenizer.word_counts;\n",
    "    wordCountDictionary = sorted(wordCountDictionary.items(), key=operator.itemgetter(1), reverse=True);\n",
    "    listOfWords = [];\n",
    "    nonCommonWords = [];\n",
    "    for w,c in wordCountDictionary:\n",
    "        if c > minNumberOfRepeated:\n",
    "            listOfWords.append(w);\n",
    "        else:\n",
    "            nonCommonWords.append(w);\n",
    "    print(\"Number of words repeated more than %d times are %d while %d non common\"%(minNumberOfRepeated, len(listOfWords), len(nonCommonWords)));\n",
    "\n",
    "    print(\"\\nLoading GloVe\");\n",
    "    glovePath = \"GloveData/\";\n",
    "    embeddingDic = {};\n",
    "    f = open(glovePath + \"tingle-vectors-\" + str(wordDimensions) + \".txt\");\n",
    "    for line in f:\n",
    "        values = line.split();\n",
    "        currrentWord = values[0];\n",
    "        currentVector = np.asarray(values[1:], dtype='float32');\n",
    "        embeddingDic[currrentWord] = currentVector;\n",
    "    f.close();\n",
    "    print('Found %d word vectors.' % len(embeddingDic));\n",
    "\n",
    "    print(\"\\nConstructing Embedding Matrix\");\n",
    "    f = open('Gabb/wordsDic2.txt', 'w')\n",
    "    embeddingMatrix = np.zeros((len(tokenizer.word_index) + 1, wordDimensions));\n",
    "    wordsNotInWiki = [];\n",
    "    reverseDic = {};\n",
    "    for w, i in tokenizer.word_index.items():\n",
    "        reverseDic[i] = w;\n",
    "        f.write(str(i) + \" \" + w + \"\\n\");\n",
    "        if w in embeddingDic:\n",
    "            embeddingMatrix[i] = embeddingDic[w];\n",
    "        else:\n",
    "            wordsNotInWiki.append(w);\n",
    "    print(\"Embedding Matrix Loaded of Dimensions: \" + str(embeddingMatrix.shape));\n",
    "    print(\"%d out of %d is not in corpus\"%(len(wordsNotInWiki), len(tokenizer.word_index)));\n",
    "    f.close();\n",
    "    \n",
    "    return [tokenizer, dataX, dataY, reverseDic, embeddingMatrix, rawText];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading tingle stories\n",
      "21612 sentences has been loaded\n",
      "134 stories has been loaded\n",
      "\n",
      "Converting input data to sequences...\n",
      "Number of stories patterns: 632470\n",
      "Number of sentence patterns: 391047\n",
      "\n",
      "Reshaping the data\n",
      "Number of words repeated more than 0 times are 12443 while 0 non common\n",
      "\n",
      "Loading GloVe\n",
      "Found 12453 word vectors.\n",
      "\n",
      "Constructing Embedding Matrix\n",
      "Embedding Matrix Loaded of Dimensions: (12444, 100)\n",
      "0 out of 12443 is not in corpus\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "('Keyword argument not understood:', 'recurrent_dropout')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-507c1da53870>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwordDimensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_input_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeSeriesLength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membeddingMatrix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropoutPercentage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mwordDimensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstateful\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurrent_dropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropoutPercentage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropoutPercentage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mwordDimensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstateful\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurrent_dropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropoutPercentage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/anaconda3/envs/py36/lib/python3.6/site-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, output_dim, init, inner_init, forget_bias_init, activation, inner_activation, W_regularizer, U_regularizer, b_regularizer, dropout_W, dropout_U, **kwargs)\u001b[0m\n\u001b[1;32m    696\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_W\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_U\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/anaconda3/envs/py36/lib/python3.6/site-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, weights, return_sequences, go_backwards, stateful, unroll, consume_less, input_dim, input_length, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_shape'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRecurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_output_shape_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Keyword argument not understood:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: ('Keyword argument not understood:', 'recurrent_dropout')"
     ]
    }
   ],
   "source": [
    "dropoutPercentage = 0.2;\n",
    "timeSeriesLength = 6;\n",
    "[tokenizer, dataX, dataY, reverseDic, embeddingMatrix, rawText] = prepareData(timeSeriesLength);\n",
    "\n",
    "model = Sequential();\n",
    "\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=wordDimensions, batch_input_shape=(batchSize, timeSeriesLength),  weights=[embeddingMatrix], trainable=True));\n",
    "model.add(Dropout(dropoutPercentage));\n",
    "model.add(LSTM(10 * wordDimensions, return_sequences=True, stateful=False, dropout_U=dropoutPercentage));\n",
    "model.add(Dropout(dropoutPercentage));\n",
    "model.add(LSTM(10 * wordDimensions, stateful=False, dropout_U=dropoutPercentage));\n",
    "model.add(Dropout(dropoutPercentage));\n",
    "model.add(Dense(10 * wordDimensions, activation='relu'));\n",
    "model.add(Dropout(dropoutPercentage));\n",
    "model.add(Dense(10 * wordDimensions, activation='relu'));\n",
    "model.add(Dropout(dropoutPercentage));\n",
    "model.add(Dense(len(tokenizer.word_index) + 1, activation='softmax', trainable=True));\n",
    "model.load_weights(\"WordWeights/TimeSeries6/tingle-dropout-2936-1.3101.hdf5\");\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001), metrics=['accuracy']);\n",
    "\n",
    "filepath=\"WordWeights/TimeSeries6/tingle-dropout-{epoch:02d}-{loss:.4f}.hdf5\";\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min');\n",
    "callbacks_list = [checkpoint];\n",
    "\n",
    "model.fit_generator(generateData(batchSize, dataX, dataY), nb_epoch=3000, samples_per_epoch=20000, callbacks=callbacks_list);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading tingle stories\n",
      "21612 sentences has been loaded\n",
      "134 stories has been loaded\n",
      "\n",
      "Converting input data to sequences...\n",
      "Number of stories patterns: 632470\n",
      "Number of sentence patterns: 391047\n",
      "\n",
      "Reshaping the data\n",
      "Number of words repeated more than 0 times are 12443 while 0 non common\n",
      "\n",
      "Loading GloVe\n",
      "Found 12453 word vectors.\n",
      "\n",
      "Constructing Embedding Matrix\n",
      "Embedding Matrix Loaded of Dimensions: (12444, 100)\n",
      "0 out of 12443 is not in corpus\n",
      "\n",
      " Story number 0:\n",
      "\n",
      "i was walking in the streets going to my friend's house . while i was walking , i stumbled upon the chamber and then heading out into the parking lot and calling my girlfriend to confirm my status as a normal, red blooded, american heterosexual. yet, despite my best efforts, i find myself getting turned on. whoa. kirk says with a laugh, sensing the hardening of my cock up against his back. you getting excited back there, buddy? no. i protest, defensively. it sure doesn't feel like it. the unicorn prods with a laugh. that feels like a big fucking human cock pressed up against my back. i don't say a word, completely embarrassed. you ever fucked a unicorn? kirk asks me suddenly. i can immediately sense a change in his tone, a new direction in his unicorn mannerisms all the way down to the way the he turns his large beastly head to speak to me. no, i can't say that i have. i explain. you're the first one i've met. kirk nods. yep, there's not a lot of us out there, not a lot of gay one's either.\n",
      "\n",
      "The end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numOfWords = 200;\n",
    "timeSeriesLength = 6;\n",
    "[tokenizer, dataX, dataY, reverseDic, embeddingMatrix, rawText] = prepareData(timeSeriesLength);\n",
    "\n",
    "model = Sequential();\n",
    "\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=wordDimensions, batch_input_shape=(1, timeSeriesLength),  weights=[embeddingMatrix], trainable=True));\n",
    "model.add(LSTM(10 * wordDimensions, return_sequences=True, stateful=False));\n",
    "model.add(LSTM(10 * wordDimensions, stateful=False));\n",
    "model.add(Dense(10 * wordDimensions, activation='relu'));\n",
    "model.add(Dense(10 * wordDimensions, activation='relu'));\n",
    "model.add(Dense(len(tokenizer.word_index) + 1, activation='softmax', trainable=True));\n",
    "\n",
    "# sentence = [dataX[80]];\n",
    "sentence = [\"i was walking in the streets going to my friend's house . while i was walking , i stumbled upon\"];\n",
    "weightsList = [\"tingle-dropout-2282-0.9320.hdf5\"];\n",
    "\n",
    "for t in range(0, len(sentence)):\n",
    "    print(\"\\n Story number \" + str(t) + \":\\n\")\n",
    "    model.load_weights(\"WordWeights/TimeSeries6/\" + weightsList[0]);\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam');\n",
    "    pattern = tokenizer.texts_to_sequences([sentence[t]]);\n",
    "    pattern = pad_sequences(pattern, maxlen=timeSeriesLength)[0];\n",
    "    previous = -1;\n",
    "    sys.stdout.write(sentence[t]);\n",
    "    i = 0;\n",
    "    c = len(sentence[t]);\n",
    "    total=sentence[t];\n",
    "    while i < numOfWords or (result != '.' and result != '?'):\n",
    "        x = np.reshape(pattern, (1, timeSeriesLength));\n",
    "        prediction = model.predict(x, verbose=0);\n",
    "        index = np.argmax(prediction);\n",
    "        result = reverseDic[index];\n",
    "        if previous != index:\n",
    "            if result in '.,;?':\n",
    "                sys.stdout.write(result);\n",
    "                total += result;\n",
    "            else:\n",
    "                sys.stdout.write(\" \" + result);\n",
    "                total += \" \" + result;\n",
    "            previous = index;\n",
    "        pattern = np.append(pattern, index);\n",
    "        pattern = pattern[1:len(pattern)];\n",
    "        i+= 1;\n",
    "        c+=len(result);\n",
    "    print(\"\\n\\nThe end\\n\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: tingle-2286-3.8420.hdf5\n",
      "Reading tingle stories\n",
      "21612 sentences has been loaded\n",
      "134 stories has been loaded\n",
      "\n",
      "Converting input data to sequences...\n",
      "Number of stories patterns: 633140\n",
      "Number of sentence patterns: 499107\n",
      "\n",
      "Reshaping the data\n",
      "Number of words repeated more than 0 times are 12443 while 0 non common\n",
      "\n",
      "Loading GloVe\n",
      "Found 12453 word vectors.\n",
      "\n",
      "Constructing Embedding Matrix\n",
      "Embedding Matrix Loaded of Dimensions: (12444, 100)\n",
      "0 out of 12443 is not in corpus\n",
      "Block 0\n",
      "\t2-gram\n",
      "\t3-gram\n",
      "\t4-gram\n",
      "\t5-gram\n",
      "\t6-gram\n",
      "\t7-gram\n",
      "\t8-gram\n",
      "\t9-gram\n",
      "Block 1\n",
      "\t2-gram\n",
      "\t3-gram\n",
      "\t4-gram\n",
      "\t5-gram\n",
      "\t6-gram\n",
      "\t7-gram\n",
      "\t8-gram\n",
      "\t9-gram\n",
      "Block 2\n",
      "\t2-gram\n",
      "\t3-gram\n",
      "\t4-gram\n"
     ]
    }
   ],
   "source": [
    "numOfWords = 200;\n",
    "weightsList = [\"tingle-2286-3.8420.hdf5\", \"tingle-2703-1.9646.hdf5\", \\\n",
    "               \"tingle-2997-0.7981.hdf5\", \"tingle-1475-0.4469.hdf5\", \\\n",
    "               \"better-fixed-tingle-0.2844.hdf5\", \"tingle-2616-0.2330.hdf5\", \\\n",
    "               \"tingle-2840-0.2099.hdf5\", \"tingle-1803-0.2181.hdf5\", \\\n",
    "               \"tingle-1794-0.2200.hdf5\", \"bigger-fixed-tingle-0.4058.hdf5\"];\n",
    "\n",
    "for w in range(0, len(weightsList)):\n",
    "    print(\"Testing: \" + weightsList[w]);\n",
    "    timeSeriesLength = w + 1;\n",
    "    [tokenizer, dataX, dataY, reverseDic, embeddingMatrix, rawText] = prepareData(timeSeriesLength);\n",
    "    model = Sequential();\n",
    "    model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=wordDimensions, batch_input_shape=(1, timeSeriesLength),  weights=[embeddingMatrix], trainable=True));\n",
    "    model.add(LSTM(10 * wordDimensions, return_sequences=True, stateful=False));\n",
    "    model.add(LSTM(10 * wordDimensions, stateful=False));\n",
    "    model.add(Dense(10 * wordDimensions, activation='relu'));\n",
    "    model.add(Dense(10 * wordDimensions, activation='relu'));\n",
    "    model.add(Dense(len(tokenizer.word_index) + 1, activation='softmax', trainable=True));\n",
    "    model.load_weights(\"WordWeights/TimeSeries\"+str(w+1)+\"/\" + weightsList[w]);\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam');\n",
    "\n",
    "    totalGrams = {};\n",
    "    exactGrams = {};\n",
    "    for t in range(100):\n",
    "        print(\"Block \" + str(t));\n",
    "        sentence = [dataX[random.randrange(len(dataX) - 1)]];\n",
    "        pattern = tokenizer.texts_to_sequences(sentence);\n",
    "        pattern = pad_sequences(pattern, maxlen=timeSeriesLength)[0];\n",
    "        previous = -1;\n",
    "        i = 0;\n",
    "        totalText = sentence[0].split();\n",
    "        while i < numOfWords:\n",
    "            x = np.reshape(pattern, (1, timeSeriesLength));\n",
    "            prediction = model.predict(x, verbose=0);\n",
    "            index = np.argmax(prediction);\n",
    "            result = reverseDic[index];\n",
    "            if previous != index:\n",
    "                totalText.append(result);\n",
    "                previous = index;\n",
    "            pattern = np.append(pattern, index);\n",
    "            pattern = pattern[1:len(pattern)];\n",
    "            i+= 1;\n",
    "        for g in range(2, 10):\n",
    "            print(\"\\t\" + str(g) + \"-gram\");\n",
    "            if g not in totalGrams:\n",
    "                totalGrams[g] = 0;\n",
    "                exactGrams[g] = 0;\n",
    "            for x in range(0, len(totalText) - g):\n",
    "                temp = \" \".join(totalText[x:x + g]);\n",
    "                if temp in rawText:\n",
    "                    exactGrams[g] += 1;\n",
    "                totalGrams[g] += 1;\n",
    "    f = open(\"grams_\" + str(timeSeriesLength) + \".txt\", \"w\");\n",
    "    for k in totalGrams:\n",
    "        f.write(str(100 * exactGrams[k] / totalGrams[k]) + \"%,\");\n",
    "    f.close();\n",
    "    \n",
    "print(\"\\n\\nThe end\\n\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Noisy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: tingle-dropout-2282-0.9320.hdf5\n",
      "Reading tingle stories\n",
      "21612 sentences has been loaded\n",
      "134 stories has been loaded\n",
      "\n",
      "Converting input data to sequences...\n",
      "Number of stories patterns: 632470\n",
      "Number of sentence patterns: 391047\n",
      "\n",
      "Reshaping the data\n",
      "Number of words repeated more than 0 times are 12443 while 0 non common\n",
      "\n",
      "Loading GloVe\n",
      "Found 12453 word vectors.\n",
      "\n",
      "Constructing Embedding Matrix\n",
      "Embedding Matrix Loaded of Dimensions: (12444, 100)\n",
      "0 out of 12443 is not in corpus\n",
      "Missing 0\n",
      "94.860798482\n",
      "Missing 1\n"
     ]
    }
   ],
   "source": [
    "weightsList = [\"tingle-dropout-2282-0.9320.hdf5\", \"tingle-2616-0.2330.hdf5\"];\n",
    "\n",
    "for t in range(0, len(weightsList)):\n",
    "    print(\"Testing: \" + weightsList[t]);\n",
    "    timeSeriesLength = 6;\n",
    "    [tokenizer, dataX, dataY, reverseDic, embeddingMatrix, rawText] = prepareData(timeSeriesLength);\n",
    "    model = Sequential();\n",
    "    model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=wordDimensions, batch_input_shape=(batchSize, timeSeriesLength), trainable=True));\n",
    "    model.add(LSTM(10 * wordDimensions, return_sequences=True, stateful=False));\n",
    "    model.add(LSTM(10 * wordDimensions, stateful=False));\n",
    "    model.add(Dense(10 * wordDimensions, activation='relu'));\n",
    "    model.add(Dense(10 * wordDimensions, activation='relu'));\n",
    "    model.add(Dense(len(tokenizer.word_index) + 1, activation='softmax', trainable=True));\n",
    "    model.load_weights(\"WordWeights/TimeSeries6/\" + weightsList[t]);\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001), metrics=['accuracy']);\n",
    "    value = [0] * 6;\n",
    "    percentage = 0;\n",
    "    for m in range(0, 6):\n",
    "        print(\"Missing \" + str(m));\n",
    "        for i in range(100):\n",
    "            value[m] += model.evaluate_generator(generateNoisyData(batchSize, math.floor(percentage * timeSeriesLength), dataX, dataY), 20000)[1]    \n",
    "        percentage += 0.2;\n",
    "        print(value[m])\n",
    "    f = open(\"data_\" + str(timeSeriesLength) + \"_\" + str(t) + \".txt\", \"w\");\n",
    "    f.write(str(value));\n",
    "    f.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Saving Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential();\n",
    "\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=wordDimensions, batch_input_shape=(1, timeSeriesLength),  weights=[embeddingMatrix], trainable=True));\n",
    "model.add(LSTM(10 * wordDimensions, return_sequences=True, stateful=False));\n",
    "model.add(LSTM(10 * wordDimensions, stateful=False));\n",
    "model.add(Dense(10 * wordDimensions, activation='relu'));\n",
    "model.add(Dense(10 * wordDimensions, activation='relu'));\n",
    "model.add(Dense(len(tokenizer.word_index) + 1, activation='softmax', trainable=True));\n",
    "model.load_weights(\"WordWeights/TimeSeries5/better-fixed-tingle-758-0.2844.hdf5\");\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001), metrics=['accuracy']);\n",
    "\n",
    "model.save_weights(\"Gabb/model.hdf5\");\n",
    "f = open(\"Gabb/model.json\", \"w\");\n",
    "f.write(model.to_json());\n",
    "f.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
