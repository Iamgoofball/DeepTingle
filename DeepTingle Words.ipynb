{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys;\n",
    "import re;\n",
    "import operator;\n",
    "import numpy as np;\n",
    "import math;\n",
    "from os import listdir, environ;\n",
    "from keras.utils import np_utils;\n",
    "from keras.regularizers import l2;\n",
    "from keras.preprocessing.text import Tokenizer;\n",
    "from keras.preprocessing.sequence import pad_sequences;\n",
    "from keras.callbacks import ModelCheckpoint;\n",
    "from keras.models import Sequential;\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, TimeDistributed, Input;\n",
    "from keras.optimizers import Adam;\n",
    "\n",
    "def cleanText(text):\n",
    "    modifiedString = re.sub(\"\\d\", \"\", text);\n",
    "    modifiedString = modifiedString.lower();\n",
    "    modifiedString = modifiedString.replace(\"\\t\", \" \");\n",
    "    modifiedString = modifiedString.replace(\"\\n\", \" \");\n",
    "    modifiedString = modifiedString.replace('!', \" . \");\n",
    "    modifiedString = modifiedString.replace('\"', \" \");\n",
    "    modifiedString = modifiedString.replace('#', \" \"); \n",
    "    modifiedString = modifiedString.replace(\"'\", \"'\"); \n",
    "    modifiedString = modifiedString.replace('(', \" \"); \n",
    "    modifiedString = modifiedString.replace(')', \" \"); \n",
    "    modifiedString = modifiedString.replace(',', \" , \");\n",
    "    modifiedString = modifiedString.replace('-', \" \"); \n",
    "    modifiedString = modifiedString.replace('.', \" . \"); \n",
    "    modifiedString = modifiedString.replace('/', \" \");\n",
    "    modifiedString = modifiedString.replace(':', \" \");\n",
    "    modifiedString = modifiedString.replace(';', \" ; \"); \n",
    "    modifiedString = modifiedString.replace('?', \" ? \");\n",
    "    modifiedString = modifiedString.replace('–', \" \");\n",
    "    modifiedString = modifiedString.replace('—', \" \");\n",
    "    modifiedString = modifiedString.replace('‘', \"'\"); \n",
    "    modifiedString = modifiedString.replace('…', \" . \");\n",
    "    modifiedString = modifiedString.replace('ç', \"c\");\n",
    "    modifiedString = modifiedString.replace('é', \"e\");\n",
    "    \n",
    "    return modifiedString;\n",
    "\n",
    "def generateData(batchSize, dataX, dataY):\n",
    "    while True:\n",
    "        randomStart = np.random.randint(math.floor((len(dataX) - batchSize) / timeSeriesLength)) * timeSeriesLength;\n",
    "        X = dataX[randomStart:randomStart+batchSize];\n",
    "        Y = dataY[randomStart:randomStart+batchSize];\n",
    "        X = tokenizer.texts_to_sequences(dataX[randomStart:randomStart+batchSize]);\n",
    "        Y = tokenizer.texts_to_sequences(dataY[randomStart:randomStart+batchSize]);\n",
    "        X = pad_sequences(X, maxlen=timeSeriesLength);\n",
    "        for temp in Y:\n",
    "            if len(temp) == 0:\n",
    "                temp.append(0);\n",
    "        Y = np_utils.to_categorical(Y, nb_classes=len(tokenizer.word_index) + 1);\n",
    "        yield (X, Y);\n",
    "        \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading tingle stories\n",
      "22214 sentences has been loaded\n",
      "134 stories has been loaded\n",
      "\n",
      "Converting input data to sequences...\n",
      "Number of stories patterns: 619874\n",
      "Number of sentence patterns: 353011\n",
      "\n",
      "Reshaping the data\n",
      "Number of words repeated more than 0 times are 12443 while 0 non common\n",
      "\n",
      "Loading GloVe\n",
      "Found 12453 word vectors.\n",
      "\n",
      "Constructing Embedding Matrix\n",
      "Embedding Matrix Loaded of Dimensions: (12444, 100)\n",
      "0 out of 12443 is not in corpus\n"
     ]
    }
   ],
   "source": [
    "timeSeriesLength = 100;\n",
    "batchSize = 100;\n",
    "minNumberOfWords = 10;\n",
    "path = \"Stories/\";\n",
    "print(\"Reading tingle stories\");\n",
    "files = [f for f in listdir(path) if (\"DS_Store\" not in f) and (\"ipynb\" not in f)];\n",
    "rawData = [];\n",
    "rawSentences = [];\n",
    "for f in files:\n",
    "    text = open(path + f).read();\n",
    "    text = cleanText(text);\n",
    "    sent = text.split(\".\");\n",
    "    for s in sent:\n",
    "        if len(s.strip().split()) > minNumberOfWords:\n",
    "            rawSentences.append(s);\n",
    "    rawData.append(text);\n",
    "print(\"%d sentences has been loaded\"%len(rawSentences));\n",
    "print(\"%d stories has been loaded\"%len(files));\n",
    "\n",
    "print(\"\\nConverting input data to sequences...\");\n",
    "dataX = [];\n",
    "dataY = [];\n",
    "for t in rawData:\n",
    "    words = t.split();\n",
    "    for k in range(0, timeSeriesLength, 1):\n",
    "        dataX.append(\" \".join(words[0:k]));\n",
    "        dataY.append(words[k]);\n",
    "        for i in range(timeSeriesLength + k, len(words) - timeSeriesLength, timeSeriesLength):\n",
    "            finalIndex = i+timeSeriesLength;\n",
    "            if i + timeSeriesLength >= len(words):\n",
    "                finalIndex = len(words) - 1;\n",
    "            dataX.append(\" \".join(words[i:finalIndex]));\n",
    "            dataY.append(words[finalIndex]);\n",
    "\n",
    "sentX = [];\n",
    "sentY = [];\n",
    "for sent in rawSentences:\n",
    "    words = sent.split();\n",
    "    words.append(\".\");\n",
    "    for i in range(0, min(timeSeriesLength, minNumberOfWords)):\n",
    "        sentX.append(\" \".join(words[0:i]));\n",
    "        sentY.append(words[i]);\n",
    "    for i in range(min(timeSeriesLength, minNumberOfWords), len(words) - min(timeSeriesLength, minNumberOfWords), 1):\n",
    "        finalIndex = i+timeSeriesLength;\n",
    "        if i + timeSeriesLength >= len(words):\n",
    "            finalIndex = len(words) - 1;\n",
    "        sentX.append(\" \".join(words[i:finalIndex]));\n",
    "        sentY.append(words[finalIndex]);\n",
    "print(\"Number of stories patterns: %d\"%len(dataX));\n",
    "print(\"Number of sentence patterns: %d\"%len(sentX));\n",
    "\n",
    "print(\"\\nReshaping the data\");\n",
    "minNumberOfRepeated = 0;\n",
    "tokenizer = Tokenizer(filters=\"\");\n",
    "tokenizer.fit_on_texts(rawData);\n",
    "wordCountDictionary = tokenizer.word_counts;\n",
    "wordCountDictionary = sorted(wordCountDictionary.items(), key=operator.itemgetter(1), reverse=True);\n",
    "listOfWords = [];\n",
    "nonCommonWords = [];\n",
    "for w,c in wordCountDictionary:\n",
    "    if c > minNumberOfRepeated:\n",
    "        listOfWords.append(w);\n",
    "    else:\n",
    "        nonCommonWords.append(w);\n",
    "print(\"Number of words repeated more than %d times are %d while %d non common\"%(minNumberOfRepeated, len(listOfWords), len(nonCommonWords)));\n",
    "\n",
    "print(\"\\nLoading GloVe\");\n",
    "wordDimensions = 100;\n",
    "glovePath = \"GloveData/\";\n",
    "embeddingDic = {};\n",
    "f = open(glovePath + \"tingle-vectors-\" + str(wordDimensions) + \".txt\");\n",
    "for line in f:\n",
    "    values = line.split();\n",
    "    currrentWord = values[0];\n",
    "    currentVector = np.asarray(values[1:], dtype='float32');\n",
    "    embeddingDic[currrentWord] = currentVector;\n",
    "f.close();\n",
    "print('Found %d word vectors.' % len(embeddingDic));\n",
    "\n",
    "print(\"\\nConstructing Embedding Matrix\");\n",
    "embeddingMatrix = np.zeros((len(tokenizer.word_index) + 1, wordDimensions));\n",
    "wordsNotInWiki = [];\n",
    "reverseDic = {};\n",
    "for w, i in tokenizer.word_index.items():\n",
    "    reverseDic[i] = w;\n",
    "    if w in embeddingDic:\n",
    "        embeddingMatrix[i] = embeddingDic[w];\n",
    "    else:\n",
    "        wordsNotInWiki.append(w);\n",
    "print(\"Embedding Matrix Loaded of Dimensions: \" + str(embeddingMatrix.shape));\n",
    "print(\"%d out of %d is not in corpus\"%(len(wordsNotInWiki), len(tokenizer.word_index)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep Tingle\n",
    "\n",
    "- Input dimension = 100\n",
    "- Trained Embedding Layer of input 100 timeseries and output of 100 dimensions\n",
    "- Not Stateful LSTM\n",
    "- using all the stories with no limit on the max number of words/with limit\n",
    "- 2 Hidden LSTM Layers each of 1000 nodes\n",
    "- Adam with learning rate 0.0001 and other are defaults\n",
    "- Batchsize 100\n",
    "- 100 Epochs\n",
    "- 20,000 samples per epoch\n",
    "- categorical crossentropy as loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (Unable to open file: name = 'wordweights/tingle-91-2.8683.hdf5', errno = 2, error message = 'no such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b0ff753c3b4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# model.add(Dropout(dropOutPercentage));\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WordWeights/tingle-91-2.8683.hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/anaconda3/envs/py35/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m   2700\u001b[0m         \"\"\"\n\u001b[1;32m   2701\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2702\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2703\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'layer_names'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'model_weights'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2704\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/anaconda3/envs/py35/lib/python3.5/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyter/anaconda3/envs/py35/lib/python3.5/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/home/ilan/minonda/conda-bld/work/h5py/_objects.c:2696)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/home/ilan/minonda/conda-bld/work/h5py/_objects.c:2654)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open (/home/ilan/minonda/conda-bld/work/h5py/h5f.c:1942)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (Unable to open file: name = 'wordweights/tingle-91-2.8683.hdf5', errno = 2, error message = 'no such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "dropOutPercentage = 0.2;\n",
    "\n",
    "model = Sequential();\n",
    "\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=wordDimensions, batch_input_shape=(100, timeSeriesLength),  weights=[embeddingMatrix], trainable=False));\n",
    "# model.add(Dropout(dropOutPercentage));\n",
    "model.add(LSTM(10 * wordDimensions, return_sequences=True, stateful=False));\n",
    "# model.add(Dropout(dropOutPercentage));\n",
    "model.add(LSTM(10 * wordDimensions, stateful=False));\n",
    "# model.add(Dropout(dropOutPercentage));\n",
    "model.add(Dense(len(tokenizer.word_index)+1, activation='softmax'));\n",
    "model.load_weights(\"WordWeights/tingle-91-2.8683.hdf5\");\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001));\n",
    "\n",
    "filepath=\"WordWeights/tingle-{epoch:02d}-{loss:.4f}.hdf5\";\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min');\n",
    "callbacks_list = [checkpoint];\n",
    "\n",
    "model.fit_generator(generateData(100, dataX, dataY), nb_epoch=100, samples_per_epoch=20000, callbacks=callbacks_list);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep Tingle to predict word dimensions\n",
    "- Same as above\n",
    "- Have a fixed final layer to convert the word Dimensions to which word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "19900/20000 [============================>.] - ETA: 0s - loss: 11.0374 - acc: 0.0015Epoch 00000: loss improved from inf to 11.03642, saving model to WordWeights/fixed-tingle-00-11.0364.hdf5\n",
      "20000/20000 [==============================] - 67s - loss: 11.0364 - acc: 0.0015    \n",
      "Epoch 2/100\n",
      " 5400/20000 [=======>......................] - ETA: 49s - loss: 10.9017 - acc: 9.2593e-04"
     ]
    }
   ],
   "source": [
    "model = Sequential();\n",
    "\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=wordDimensions, batch_input_shape=(batchSize, timeSeriesLength),  weights=[embeddingMatrix], trainable=False));\n",
    "model.add(LSTM(10 * wordDimensions, return_sequences=True, stateful=False));\n",
    "model.add(LSTM(10 * wordDimensions, stateful=False));\n",
    "model.add(Dense(wordDimensions, activation='linear'));\n",
    "\n",
    "fixedModel = Sequential();\n",
    "fixedModel.add(Dense(10 * wordDimensions, batch_input_shape=(None, wordDimensions), activation='relu', trainable=False));\n",
    "fixedModel.add(Dense(len(tokenizer.word_index) + 1, activation='softmax', trainable=False));\n",
    "fixedModel.load_weights(\"GloVeToCategorical/converter-tokenizer-0.3000.hdf5\");\n",
    "\n",
    "model.add(fixedModel);\n",
    "model.load_weights(\"WordWeights/fixed-tingle-98-9.5278.hdf5\");\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001), metrics=['accuracy']);\n",
    "\n",
    "filepath=\"WordWeights/fixed-tingle-{epoch:02d}-{loss:.4f}.hdf5\";\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min');\n",
    "callbacks_list = [checkpoint];\n",
    "\n",
    "model.fit_generator(generateData(batchSize, dataX, dataY), nb_epoch=100, samples_per_epoch=20000, callbacks=callbacks_list);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep Tingle\n",
    "\n",
    "- Same like the previous but the data set now is not stories but sentences. \n",
    "- Sequence length is 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 7300/20000 [=========>....................] - ETA: 7s - loss: 6.0856"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-495e8fb92e0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mcallbacks_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerateData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamples_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/jupyter/anaconda3/envs/py35/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    933\u001b[0m                                         \u001b[0mnb_worker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 935\u001b[1;33m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    936\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m     def evaluate_generator(self, generator, val_samples,\n",
      "\u001b[1;32m/home/jupyter/anaconda3/envs/py35/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch)\u001b[0m\n\u001b[0;32m   1555\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   1556\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1557\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   1558\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1559\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jupyter/anaconda3/envs/py35/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1318\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1320\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1321\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jupyter/anaconda3/envs/py35/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1941\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1942\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m-> 1943\u001b[1;33m                               feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m   1944\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jupyter/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    764\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 766\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    767\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jupyter/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    962\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 964\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    965\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jupyter/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1014\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1015\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/home/jupyter/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1019\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1020\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1022\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jupyter/anaconda3/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1003\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1004\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dropOutPercentage = 0.2;\n",
    "\n",
    "model = Sequential();\n",
    "\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=wordDimensions, batch_input_shape=(100, timeSeriesLength),  weights=[embeddingMatrix], trainable=False));\n",
    "# model.add(Dropout(dropOutPercentage));\n",
    "model.add(LSTM(10 * wordDimensions, return_sequences=True, stateful=False));\n",
    "# model.add(Dropout(dropOutPercentage));\n",
    "model.add(LSTM(10 * wordDimensions, stateful=False));\n",
    "# model.add(Dropout(dropOutPercentage));\n",
    "model.add(Dense(len(tokenizer.word_index)+1, activation='softmax'));\n",
    "model.load_weights(\"WordWeights/tingle-common-1000x1000-3.5542.hdf5\");\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001));\n",
    "\n",
    "filepath=\"WordWeights/tingle-{epoch:02d}-{loss:.4f}.hdf5\";\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min');\n",
    "callbacks_list = [checkpoint];\n",
    "\n",
    "model.fit_generator(generateData(100, sentX, sentY), nb_epoch=100, samples_per_epoch=20000, callbacks=callbacks_list);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate New Story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can suck his unicorn dick with who second of it is just as away as the rest of my absurd begin to insist across the air . i know that this is my own room say would truly before but my life is a prehistoric . it's been a long short and a long time i finally tell the handsome desk in the passenger microphone . i look up at the towering turns with a cops wink . he keys a remember new and then he is right up next to me . i look up at him with a wink opens in his eyes velbot eyes . what do you want to do to this . i ask my voice trembling . i mean . i tell him . i need to be wiping . i tell him . i need to be wiping a mashly . i tell him . i jeans to him . so . i mean . i mean it's just a lot but it was a mashly . my attention is fellow with ourselves . the dinosaur haunted . you don't have the job to fuck me . i ask . i mean it was a lot but it was a who's place to be beginning . \n",
      "\n",
      "The end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numOfWords = 200;\n",
    "\n",
    "model = Sequential();\n",
    "\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=wordDimensions, batch_input_shape=(1, timeSeriesLength),  weights=[embeddingMatrix]));\n",
    "model.add(LSTM(10 * wordDimensions, return_sequences=True, stateful=False));\n",
    "model.add(LSTM(10 * wordDimensions, stateful=False));\n",
    "model.add(Dense(len(tokenizer.word_index)+1, activation='softmax'));\n",
    "model.load_weights(\"WordWeights/tingle-common-sent-2xStory-2.2805.hdf5\");\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam');\n",
    "\n",
    "sentence = [\"I can suck his unicorn dick\"];\n",
    "pattern = tokenizer.texts_to_sequences(sentence);\n",
    "pattern = pad_sequences(pattern, maxlen=timeSeriesLength)[0];\n",
    "previous = -1;\n",
    "sys.stdout.write(sentence[0] + \" \");\n",
    "i = 0;\n",
    "while i < numOfWords or result != '.':\n",
    "    x = np.reshape(pattern, (1, timeSeriesLength));\n",
    "    prediction = model.predict(x, verbose=0);\n",
    "    index = np.argmax(prediction);\n",
    "    result = reverseDic[index];\n",
    "    if previous != index:\n",
    "        sys.stdout.write(result + \" \");\n",
    "        previous = index;\n",
    "    pattern = np.append(pattern, index);\n",
    "    pattern = pattern[1:len(pattern)];\n",
    "    i+= 1;\n",
    "    \n",
    "print(\"\\n\\nThe end\\n\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Fixed Story\n",
    "- fixed-tingle-97-3.5204.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can suck his unicorn ass systematically up slapped systematically crisscrossed stupor systematically crisscrossed stupor systematically crisscrossed stupor systematically crisscrossed stupor crisscrossed stupor looking at triceratopses marketplace now addision now want colleagues conquers clattering newspaper mankind's outlawed crater bedroll systematically crisscrossed drills slapped systematically crisscrossed stupor systematically crisscrossed stupor systematically crisscrossed stupor crisscrossed stupor systematically crisscrossed stupor crisscrossed stupor crisscrossed stupor looking at triceratopses announces marketplace just much \n",
      "\n",
      "The end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numOfWords = 200;\n",
    "\n",
    "model = Sequential();\n",
    "\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=wordDimensions, batch_input_shape=(1, timeSeriesLength),  weights=[embeddingMatrix], trainable=False));\n",
    "model.add(LSTM(10 * wordDimensions, return_sequences=True, stateful=False));\n",
    "model.add(LSTM(10 * wordDimensions, stateful=False));\n",
    "model.add(Dense(wordDimensions, activation='linear'));\n",
    "\n",
    "fixedModel = Sequential();\n",
    "fixedModel.add(Dense(10 * wordDimensions, batch_input_shape=(None, wordDimensions), activation='relu', trainable=False));\n",
    "fixedModel.add(Dense(len(tokenizer.word_index) + 1, activation='softmax', trainable=False));\n",
    "fixedModel.load_weights(\"GloVeToCategorical/converter-tokenizer-0.3000.hdf5\");\n",
    "\n",
    "model.add(fixedModel);\n",
    "model.load_weights(\"WordWeights/fixed-tingle-98-8.6490.hdf5\");\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001));\n",
    "\n",
    "sentence = [\"I can suck his unicorn ass\"];\n",
    "pattern = tokenizer.texts_to_sequences(sentence);\n",
    "pattern = pad_sequences(pattern, maxlen=timeSeriesLength)[0];\n",
    "previous = -1;\n",
    "sys.stdout.write(sentence[0] + \" \");\n",
    "i = 0;\n",
    "while i < numOfWords:\n",
    "    x = np.reshape(pattern, (1, timeSeriesLength));\n",
    "    prediction = model.predict(x, verbose=0);\n",
    "    index = np.argmax(prediction);\n",
    "    result = reverseDic[index];\n",
    "    if previous != index:\n",
    "        sys.stdout.write(result + \" \");\n",
    "        previous = index;\n",
    "    pattern = np.append(pattern, index);\n",
    "    pattern = pattern[1:len(pattern)];\n",
    "    i+= 1;\n",
    "    \n",
    "print(\"\\n\\nThe end\\n\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.29761600971222, 0.056799997761845591]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential();\n",
    "\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=wordDimensions, batch_input_shape=(100, timeSeriesLength),  weights=[embeddingMatrix], trainable=False));\n",
    "model.add(LSTM(10 * wordDimensions, return_sequences=True, stateful=False));\n",
    "model.add(LSTM(10 * wordDimensions, stateful=False));\n",
    "model.add(Dense(wordDimensions, activation='linear'));\n",
    "\n",
    "fixedModel = Sequential();\n",
    "fixedModel.add(Dense(10 * wordDimensions, batch_input_shape=(None, wordDimensions), activation='relu', trainable=False));\n",
    "fixedModel.add(Dense(len(tokenizer.word_index) + 1, activation='softmax', trainable=False));\n",
    "fixedModel.load_weights(\"GloVeToCategorical/converter-tokenizer-0.3000.hdf5\");\n",
    "\n",
    "model.add(fixedModel);\n",
    "model.load_weights(\"WordWeights/fixed-tingle-98-8.6490.hdf5\");\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001), metrics=['accuracy']);\n",
    "\n",
    "print(model.evaluate_generator(generateData(100, dataX, dataY), 20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
